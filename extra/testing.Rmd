---
title: "average rating ~ playtime"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r load-packages, message = FALSE}
library(tidyverse)
library(broom)
library(pander)
library(here)
library(tidymodels)
```

```{r setup, include=FALSE}
board_games <- read_csv(here("data/board_games.csv"))
```

## Initial PLots

```{r plot}
board_games %>%
  ggplot(aes(playing_time, average_rating)) +
  geom_point(alpha = 0.6)
```

We can see that this plot is fairly useless as the x axis is stretched very far because there is a game with a playtime of 60000 minutes. To counter this I will log the x axis.
```{r no_playtime, echo = FALSE}
no_playtime <- board_games %>%
  filter(playing_time == 0) %>%
  arrange(desc(average_rating))
```

```{r log_scale}
board_games %>%
  filter(playing_time != 0) %>%
  ggplot(aes(playing_time, average_rating)) +
  geom_point(alpha = 0.6) +
  scale_x_log10() +
  labs(title = "Average Rating against Playing Time", x = "Playing Time", y = "Average Rating")
```

After manipulating the axes and filtering out games with a `playing_time` = 0, we can see that there is a slight positive correlation between the `playing_time` and `average_rating`.

## Splitting Data

Here I just want to split my data into training data and testing data so that I can experiment and work on my model before lettin git model all of the data to avoid overfitting the model.

```{r boarg_games_split}
board_games <- board_games %>%
  filter(playing_time != 0, min_playtime != 0)

set.seed(1116)

games_split <- initial_split(board_games, prop = 0.8)
train_data <- training(games_split)
test_data <- testing(games_split) 

# splitting data into training/test data
```

## Creating Models

Here I am creating two simple linear regression models between `playing_time` and `average_rating`, and `playing_time`, `min_playtime` and `average_rating`. I puprosely left out `max_playtime` because in the data `playing_time` and `max_playtime` are always the same, meaning that adding the `max_playtime` would be irrelevant.

```{r model}
games_lm_mod <- linear_reg() %>%
  set_engine("lm")

games_play_recipe <- recipe(average_rating ~ playing_time, data = train_data) %>%
  step_log(all_predictors())

# for the same reason I logged the x axis in the second plot,
# I am logging the data to make the plot nicer

games_play_workflow <- workflow() %>%
  add_model(games_lm_mod) %>%
  add_recipe(games_play_recipe)

# here I am fitting the model to the train data

games_play_fit <- games_play_workflow %>% 
  fit(data = train_data)

# Here I am doing exactly the same process as before
# the only difference is that I am including `min_playtime` in the model now

games_play_all_recipe <- recipe(
  average_rating ~ playing_time + min_playtime,
  data = train_data
  ) %>%
  step_log(all_predictors())

games_play_all_workflow <- workflow() %>%
  add_model(games_lm_mod) %>%
  add_recipe(games_play_all_recipe)

games_play_all_fit <- games_play_all_workflow %>%
  fit(data = train_data)
```

```{r predict}
# this code is creating a data frame of the predicted
# average ratings based on the model with only the
# `playing_time` variable

games_play_pred <- predict(games_play_fit, test_data) %>%
  bind_cols(test_data)

games_play_pred %>%
  ggplot(aes(x = playing_time)) +
  geom_point(aes(y = average_rating)) +
  geom_line(aes(y = .pred), colour = "red") +
  scale_x_log10() +
  labs(x = "Average Playing Time", y = "Average Rating")

# this code is doing the same as above, creating a data frame
# of the predicted average values from the model with 
# `playing_time` and `min_playtime`

games_play_all_pred <- predict(games_play_all_fit, test_data) %>%
  bind_cols(test_data)

# here I am joining the data frames so I am able to see the predicted
# average rating from the simple model and the model with both
# variables in one place compared to the actual `average_rating`

predict_both <- games_play_all_pred %>%
  mutate(.pred_all = .pred, .keep = "unused") %>%
  right_join(games_play_pred) %>%
  select(.pred, .pred_all, average_rating, everything())

predict_both %>%
    ggplot(aes(x = playing_time)) +
  geom_point(aes(y = average_rating)) +
  geom_line(aes(y = .pred), colour = "red") +
  scale_x_log10() +
  labs(x = "Average Playing Time", y = "Average Rating")
```

## Evaluating my models

To evaluate the models I used chi squared and root mean squared error (RMSE). 
```{r evaluating-models}
rmse(games_play_pred, average_rating, .pred)

#evaluating my average_rating ~ playing_time model using root mean square error

rmse(games_play_all_pred, average_rating, .pred)

#evaluating my average_rating ~ playing_time + min_playtime model using chi squared and root mean square error
```

```{r v-fold_validation, echo = FALSE}
set.seed(450)

folds <- vfold_cv(train_data, v = 50)

# fit re-sampling

set.seed(451)

games_play_fit_rs <- games_play_workflow %>%
  fit_resamples(folds)

games_play_all_fit_rs <- games_play_all_workflow %>%
  fit_resamples(folds)
```

Evaluating my model with only `playing_time` variable\
```{r evaluate-first, echo = FALSE}
collect_metrics(games_play_fit_rs)

glance(games_play_fit %>% pull_workflow_fit())
```

evaluating my model with `playing_time` and `min_playtime` variables

```{r evaluate-second, echo = FALSE}
collect_metrics(games_play_all_fit_rs)

glance(games_play_all_fit %>% pull_workflow_fit())
```


We can see from comparing the 2 RMSE and adjusted r squared mean values that the model that includes `min_playtime` as well as `playing_time` is a better model, however this does not mean that either are good models. 