---
title: "users_rated"
author: "Arjun NR"
date: "11/21/2020"
output: github_document
editor_options: 
  chunk_output_type: console
---

## Loading packages
```{r load packages}
library(tidyverse)
library(broom)
library(pander)
library(here)
library(tidymodels)
library(fitdistrplus)
```

## Relevant work from proposal
```{r load-data, message = FALSE}
board_games <- read_csv(here("data/board_games.csv"))
```

```{r popular-categories, include = FALSE}
board_games_splitcats <- board_games %>% 
  mutate(categories = str_split(category, ","))

popular_categories <- board_games_splitcats %>%
  pull(categories) %>%
  unlist %>%
  as_tibble %>%
  count(value) %>%
  arrange(desc(n)) %>%
  head(6)
```

```{r no_ratings-v-rating, warning = FALSE}
board_games_splitcats %>%
  ggplot() +
  geom_point(aes(x = users_rated, y = average_rating), alpha = 0.1) +
  # scale_x_log10() +
  labs(
  x = "Number of ratings",
  y = "Average ratings",
  title = "Number of ratings against average rating"
  )
```

This plot suggests that games with more ratings tend to have an average rating closer to just under 7.5. However, this may be because there are more games with a rating of 7.5, which increases the likelihood that there will be a game with particularly a large number of ratings. To analyze this further, we will need to eliminate the effect of this confounding variable (number of games with a given rating) to properly analyze the relationship between the two variables. 

## Further work

```{r plus-x-y-densities}
board_games_splitcats %>%
  ggplot() +
  geom_point(aes(x = users_rated, y = average_rating), alpha = 0.05) +
  geom_density(aes(x = users_rated, y = (..scaled.. * 5)), colour = "red") +
  geom_density(aes(x = (..scaled.. * 1e4), y = average_rating), colour = "orange") +
  scale_x_log10() +
  labs(
  x = "Number of ratings",
  y = "Average ratings",
  title = "Number of ratings against average rating", 
  subtitle = "Showing density plots of x and y overlayed"
  ) + 
  scale_colour_viridis_c()
  # xlim(NA, 20000)
```

Below, I have attempted to create a 3D plot of average_rating against users_rated against year_published. It was not particularly informative. 

```{r 3d-plot-n_rate-avge-year, eval=FALSE}
library(plotly)

board_games_splitcats %>% 
  plot_ly(x = ~users_rated, y = ~average_rating, z = ~year_published, type = "scatter3d", mode = "markers", marker = list(color = ~year_published, colorscale = c('#FFE1A1', '#683531'), showscale = TRUE), size = 1, opacity = 0.1)
# plot_ly(x=temp, y=pressure, z=dtime, type="scatter3d", mode="markers", color=temp)
```

### Modelling average_rating from users_rated

```{r av-n-rating-model}
set.seed(314159)
bg_nrate_split <- initial_split(board_games_splitcats, prop = 0.8)
train_data <- training(bg_nrate_split)
test_data <- testing(bg_nrate_split)

bg_nrate_rec <- recipe(
  average_rating ~ users_rated,
  data = train_data
) %>% 
  step_log(users_rated)

bg_nrate_model <- linear_reg() %>% 
  set_engine("lm")

bg_nrate_wflow <- workflow() %>% 
  add_model(bg_nrate_model) %>% 
  add_recipe(bg_nrate_rec)

bg_nrate_fit <- bg_nrate_wflow %>%
  fit(data = train_data)

bg_nrate_fit_tidy <- tidy(bg_nrate_fit)
# bg_nrate_fit_aug <- augment(bg_nrate_fit) # Why does this break?

# Evaluation
set.seed(314159)
folds <- vfold_cv(train_data, v = 5)

bg_nrate_fit_vfold <- bg_nrate_wflow %>% 
  fit_resamples(folds)

# Metrics for model fitted to v-fold train data: 
train_metrics <- collect_metrics(bg_nrate_fit_vfold)

# predictions and metrics for test data
bg_nrate_pred <- predict(bg_nrate_fit, test_data) %>% 
  bind_cols(
    predict(bg_nrate_fit, test_data, type = "pred_int"),
    test_data %>% dplyr::select(average_rating, users_rated, name)
  )

test_rmse <- rmse(bg_nrate_pred, truth = average_rating, estimate = .pred)
test_rsq <- rsq(bg_nrate_pred, truth = average_rating, estimate = .pred)

# tibble of metrics for both train and test data
train_test_metrics <- tribble(
  ~data, ~metric, ~value,
  "train", "rmse", train_metrics$mean[1],
  "train", "rsq", train_metrics$mean[2],
  "test", "rmse", test_rmse$.estimate[1],
  "test", "rsq", test_rsq$.estimate[1],
) %>% 
  pivot_wider(names_from = data, values_from = value)

# plot of model on full data
predict(bg_nrate_fit, board_games_splitcats) %>% 
  bind_cols(
    predict(bg_nrate_fit, board_games_splitcats, type = "pred_int"),
    board_games_splitcats %>% dplyr::select(average_rating, users_rated, name)
  ) %>% 
  ggplot(aes(x = users_rated)) + 
  geom_point(aes(y = average_rating), alpha = 0.2) + 
  geom_line(aes(y = .pred), colour = "red") + 
  geom_line(aes(y = .pred_upper), colour = "dark red", linetype = "dashed") +
  geom_line(aes(y = .pred_lower), colour = "dark red", linetype = "dashed") + 
  scale_x_log10()
```

```{r av-n-rating-model-plot-logy}
set.seed(314159)
bg_nrate_split <- initial_split(board_games_splitcats, prop = 0.8)
train_data <- training(bg_nrate_split)
test_data <- testing(bg_nrate_split)

bg_nrate_rec <- recipe(
  average_rating ~ users_rated,
  data = train_data
) %>% 
  step_log(users_rated)

bg_nrate_model <- linear_reg() %>% 
  set_engine("lm")

bg_nrate_wflow <- workflow() %>% 
  add_model(bg_nrate_model) %>% 
  add_recipe(bg_nrate_rec)

bg_nrate_fit <- bg_nrate_wflow %>%
  fit(data = train_data)

bg_nrate_fit_tidy <- tidy(bg_nrate_fit)
# bg_nrate_fit_aug <- augment(bg_nrate_fit) # Why does this break?

# Evaluation
set.seed(314159)
folds <- vfold_cv(train_data, v = 5)

bg_nrate_fit_vfold <- bg_nrate_wflow %>% 
  fit_resamples(folds)

# Metrics for model fitted to v-fold train data: 
train_metrics <- collect_metrics(bg_nrate_fit_vfold)

# predictions and metrics for test data
bg_nrate_pred <- predict(bg_nrate_fit, test_data) %>% 
  bind_cols(
    predict(bg_nrate_fit, test_data, type = "pred_int"),
    test_data %>% dplyr::select(average_rating, users_rated, name)
  )

test_rmse <- rmse(bg_nrate_pred, truth = average_rating, estimate = .pred)
test_rsq <- rsq(bg_nrate_pred, truth = average_rating, estimate = .pred)

# tibble of metrics for both train and test data
train_test_metrics <- tribble(
  ~data, ~metric, ~value,
  "train", "rmse", train_metrics$mean[1],
  "train", "rsq", train_metrics$mean[2],
  "test", "rmse", test_rmse$.estimate[1],
  "test", "rsq", test_rsq$.estimate[1],
) %>% 
  pivot_wider(names_from = data, values_from = value)

# plot of model on full data
model_and_intervals <- predict(bg_nrate_fit, board_games_splitcats) %>% 
  bind_cols(
    predict(bg_nrate_fit, board_games_splitcats, type = "pred_int"),
    board_games_splitcats %>% dplyr::select(average_rating, users_rated, name)
  )

library(patchwork)

p1 <- model_and_intervals %>% 
  ggplot(aes(x = users_rated)) + 
  geom_point(aes(y = average_rating), alpha = 0.2) + 
  geom_line(aes(y = .pred), colour = "red") + 
  geom_line(aes(y = .pred_upper), colour = "dark red", linetype = "dashed") +
  geom_line(aes(y = .pred_lower), colour = "dark red", linetype = "dashed") + 
  scale_x_continuous(trans = "log10")

p2 <- model_and_intervals %>% 
  ggplot(aes(x = users_rated)) + 
  geom_point(aes(y = average_rating), alpha = 0.2) + 
  geom_line(aes(y = .pred), colour = "red") + 
  geom_line(aes(y = .pred_upper), colour = "dark red", linetype = "dashed") +
  geom_line(aes(y = .pred_lower), colour = "dark red", linetype = "dashed") + 
  scale_x_log10() + 
  scale_y_log10()

# p1 + p2
```

```{r av-n-rating-model-logy, eval = FALSE}
# Now trying tansforming y in model
set.seed(314159)
bg_nrate_split <- initial_split(board_games_splitcats, prop = 0.8)
train_data <- training(bg_nrate_split)
test_data <- testing(bg_nrate_split)

bg_nrate_rec <- recipe(
  average_rating ~ users_rated,
  data = train_data
) %>% 
  step_log(users_rated, average_rating)

bg_nrate_model <- linear_reg() %>% 
  set_engine("lm")

bg_nrate_wflow <- workflow() %>% 
  add_model(bg_nrate_model) %>% 
  add_recipe(bg_nrate_rec)

bg_nrate_fit <- bg_nrate_wflow %>%
  fit(data = train_data)

bg_nrate_fit_tidy <- tidy(bg_nrate_fit)
# bg_nrate_fit_aug <- augment(bg_nrate_fit) # Why does this break?

# Evaluation
set.seed(314159)
folds <- vfold_cv(train_data, v = 5)

bg_nrate_fit_vfold <- bg_nrate_wflow %>% 
  fit_resamples(folds)

# Metrics for model fitted to v-fold train data: 
train_metrics <- collect_metrics(bg_nrate_fit_vfold)

# predictions and metrics for test data
bg_nrate_pred <- predict(bg_nrate_fit, test_data) %>% 
  bind_cols(
    predict(bg_nrate_fit, test_data, type = "pred_int"),
    test_data %>% dplyr::select(average_rating, users_rated, name)
  )

test_rmse <- rmse(bg_nrate_pred, truth = average_rating, estimate = .pred)
test_rsq <- rsq(bg_nrate_pred, truth = average_rating, estimate = .pred)

# tibble of metrics for both train and test data
train_test_metrics_logy <- tribble(
  ~data, ~metric, ~value,
  "train", "rmse", train_metrics$mean[1],
  "train", "rsq", train_metrics$mean[2],
  "test", "rmse", test_rmse$.estimate[1],
  "test", "rsq", test_rsq$.estimate[1],
) %>% 
  pivot_wider(names_from = data, values_from = value)

# plot of model on full data
model_and_intervals <- predict(bg_nrate_fit, board_games_splitcats) %>% 
  bind_cols(
    predict(bg_nrate_fit, board_games_splitcats, type = "pred_int"),
    board_games_splitcats %>% dplyr::select(average_rating, users_rated, name)
  )

p3 <- model_and_intervals %>% 
  ggplot(aes(x = users_rated)) + 
  geom_point(aes(y = average_rating), alpha = 0.2) + 
  geom_line(aes(y = .pred), colour = "red") + 
  geom_line(aes(y = .pred_upper), colour = "dark red", linetype = "dashed") +
  geom_line(aes(y = .pred_lower), colour = "dark red", linetype = "dashed") + 
  scale_x_continuous(trans = "log10")

p4 <- model_and_intervals %>% 
  ggplot(aes(x = users_rated)) + 
  geom_point(aes(y = average_rating), alpha = 0.2) + 
  geom_line(aes(y = .pred), colour = "red") + 
  geom_line(aes(y = .pred_upper), colour = "dark red", linetype = "dashed") +
  geom_line(aes(y = .pred_lower), colour = "dark red", linetype = "dashed") + 
  scale_x_log10() + 
  scale_y_log10()

p1 + p2 +
  p3 + p4

train_test_metrics
train_test_metrics_logy
```

### Modelling with all

```{r from-bruce}
board_games_splitcats <- board_games %>% 
  mutate(categories = str_split(category, ","))

popular_categories <- board_games_splitcats %>%
  pull(categories) %>%
  unlist %>%
  as_tibble %>%
  count(value) %>%
  arrange(desc(n)) %>%
  head(6) %>%
  pull(value)


board_games_empty <- board_games_splitcats %>%
  filter(FALSE)
board_games_popcats <- board_games_empty

for (c in popular_categories) {
  board_games_popcats <- full_join(
    board_games_popcats,
    board_games_splitcats %>%
      filter(map_lgl(categories, ~c %in% .x)) %>%
      mutate(category = c)
  )
}
```

Modelling with all four variables we are considering: 
```{r model-with-all}
set.seed(314159)
bg_split <- initial_split(board_games_popcats, prop = 0.8)
train_data <- training(bg_split)
test_data <- testing(bg_split)


bg_rec <- recipe(
  average_rating ~ users_rated + category + year_published + playing_time,
  data = train_data
) %>% 
  step_log(users_rated)

bg_model <- linear_reg() %>% 
  set_engine("lm")

bg_wflow <- workflow() %>% 
  add_model(bg_model) %>% 
  add_recipe(bg_rec)

bg_fit <- bg_wflow %>%
  fit(data = train_data)

bg_fit_tidy <- tidy(bg_fit)

bg_fit_tidy

# Evaluation
set.seed(314159)
folds <- vfold_cv(train_data, v = 5)

bg_fit_vfold <- bg_wflow %>% 
  fit_resamples(folds)

# Metrics for model fitted to v-fold train data: 
bg_train_metrics <- collect_metrics(bg_fit_vfold)

# predictions and metrics for test data
bg_pred <- predict(bg_fit, test_data) %>% 
  bind_cols(
    predict(bg_fit, test_data, type = "pred_int"),
    test_data %>% dplyr::select(average_rating, users_rated, name)
  )

bg_test_rmse <- rmse(bg_nrate_pred, truth = average_rating, estimate = .pred)
bg_test_rsq <- rsq(bg_nrate_pred, truth = average_rating, estimate = .pred)

bg_train_adj_rsq <- glance(bg_fit %>% pull_workflow_fit)$adj.r.squared[1]

# tibble of metrics for both train and test data
bg_train_test_metrics <- tribble(
  ~data, ~metric, ~value,
  "train", "rmse", bg_train_metrics$mean[1],
  "train", "rsq", bg_train_metrics$mean[2],
  "test", "rmse", bg_test_rmse$.estimate[1],
  "test", "rsq", bg_test_rsq$.estimate[1],
  "train", "adj rsq", bg_train_adj_rsq
) %>% 
  pivot_wider(names_from = data, values_from = value)

bg_train_test_metrics
```

And if we include minimum age: 
```{r model-with-all-and-min-age}
set.seed(314159)
bg_split <- initial_split(board_games_popcats, prop = 0.8)
train_data <- training(bg_split)
test_data <- testing(bg_split)


bg_rec <- recipe(
  average_rating ~ users_rated + category + year_published + playing_time + min_age,
  data = train_data
) %>% 
  step_log(users_rated)

bg_model <- linear_reg() %>% 
  set_engine("lm")

bg_wflow <- workflow() %>% 
  add_model(bg_model) %>% 
  add_recipe(bg_rec)

bg_fit <- bg_wflow %>%
  fit(data = train_data)

bg_fit_tidy <- tidy(bg_fit)

bg_fit_tidy

# Evaluation
set.seed(314159)
folds <- vfold_cv(train_data, v = 5)

bg_fit_vfold <- bg_wflow %>% 
  fit_resamples(folds)

# Metrics for model fitted to v-fold train data: 
bg_train_metrics <- collect_metrics(bg_fit_vfold)

# predictions and metrics for test data
bg_pred <- predict(bg_fit, test_data) %>% 
  bind_cols(
    predict(bg_fit, test_data, type = "pred_int"),
    test_data %>% dplyr::select(average_rating, users_rated, name)
  )

bg_test_rmse <- rmse(bg_nrate_pred, truth = average_rating, estimate = .pred)
bg_test_rsq <- rsq(bg_nrate_pred, truth = average_rating, estimate = .pred)

bg_train_adj_rsq <- glance(bg_fit %>% pull_workflow_fit)$adj.r.squared[1]

# tibble of metrics for both train and test data
bg_train_test_metrics <- tribble(
  ~data, ~metric, ~value,
  "train", "rmse", bg_train_metrics$mean[1],
  "train", "rsq", bg_train_metrics$mean[2],
  "test", "rmse", bg_test_rmse$.estimate[1],
  "test", "rsq", bg_test_rsq$.estimate[1],
  "train", "adj rsq", bg_train_adj_rsq
) %>% 
  pivot_wider(names_from = data, values_from = value)

bg_train_test_metrics
```



### Fitting a statistical distribution to the distribution of average_rating values

```{r avge_rating-dist-models}
my_data <- board_games_splitcats$average_rating
descdist(my_data, discrete=FALSE, boot=500)

fit <- function(data, distr) {
  fit <- fitdist(data, distr)
  
  print(summary(fit))
  
  return(fit)
}

fit_n  <- fit(my_data, "norm")
fit_ln  <- fit(my_data, "lnorm")
fit_l  <- fit(my_data, "logis")
fit_g  <- fit(my_data, "gamma")


# The following code is based on (Neto, 2015)
plot.legend <- c("normal", "lognormal", "logistic", "gamma")

denscomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

cdfcomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

qqcomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

ppcomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

gofstat(list(fit_n, fit_ln, fit_g, fit_l), fitnames = c("norm", "lnorm", "gamma", "logis"))
```

```{r avge_rating-dist-models-2}
my_data <- board_games_splitcats$average_rating

fit <- function(data, distr) {
  fit <- fitdist(data, distr)
  
  print(summary(fit))
  
  return(fit)
}

fit_n  <- fit(my_data, "norm")

gofstat(fit_n, fitnames = "norm")
```

```{r avge_rating-dist-plot}
sech <- function(x) {
  1 / cosh(x)
}

logis <- function(x, u, s) {
  y <- (1 / (4 * s)) * (sech((x - u) / (2 * s)))^2
  return(y)
} # inspired by code from (Sauer, 2017)

logis_av_rating <- function(x) {
  logis(x, fit_l$estimate[1], fit_l$estimate[2])
}

board_games_splitcats %>% 
  ggplot(aes(x = average_rating)) + 
  geom_density() + 
  stat_function(
    fun = dnorm, 
    args = list(
      mean = fit_n$estimate[1], 
      sd = fit_n$estimate[2]
    ), 
    colour = "red", 
    alpha = 0.5
  ) + 
  stat_function(fun = logis_av_rating, colour = "orange", alpha = 0.5) + 
  theme_minimal() + 
  labs(
    title = "Fitting distributtions to distribution of average ratings", 
    subtitle = "red = normal; orange = logistic"
  )
```


```{r notes, eval = FALSE}

```

```{r try}
paste0(
  "$$R^2=",
  round(122.222323, 3),
  ",R_{adj}^2=",
  round(3.14159, 3),
  ".$$"
  )
```



$ = 2$

### Modelling distribution of average_rating from users_rated

What I am trying to do in next chunk is model the distribution of average_rating depending on users_rated. Thus the steps will likely involve: 

1. Bin users_rated (unless I can do it in a moving average style)
2. Fit normal (or logistic) distr. to average_rating density of each bin (ie. find y-bar and s_y^2)
3. Model normal distr. as function of users_rated (ie. modelling y-bar and s_y^2 as functions of users_rated)

Hopefully will then have a model which gives, for each users_rated, a probability distribution of average_rating

```{r av-n-rating-distr-model, eval = FALSE}
# eval = FALSE as code not yet functioning


# Unfinished
## Is this the right way of going about it? need to look in to this more

#Feedback:
##in predict() can have confidence interval for each value vs the average - this will give some indication of distribution around the line. Not necessarily good idea to use sd to predict normal distribution as then you are imposing the distribution (guessing it) rather than obtaining it. 
#Can say in project that this is something we could do further (predicting distribution) if give reasons, even if don't necessarily do it. 
#estimating a prior; bayesian distributions
```

### Presentation plan:
#### Relationship between `averege_rating` and `users_rated`
- average ratings vs number of ratings graph

For low numbers of ratings, there is very high variation in average ratings, however as the number of ratings increases, the variation reduces significantly, with average ratings being close to 7.5. It appears that heavily rated games have tend be well (but not the highest) rated. This could be because good games are likely to be played and rated more while games that are highest rated are more specialist or complicated and so despite being good games, are played by fewer people. With data on complexity of the games, this would be something that could be explored.

```{r top-rated-games}
board_games_splitcats %>% 
  filter(rank(desc(average_rating)) <= 10) %>% 
  arrange(desc(average_rating)) %>% 
  dplyr::select(name, average_rating)
```

Alternatively, it may be that most games are liked by some and disliked by others (after all, games are definitely a matter of taste) in such a way that given a high enough number of ratings, all games would tend to have an average rating close to ??[value]7.5, but that a small sample size for calculating the average_rating means that ratings vary more. 

#### Distribution of average ratings
In fact, if we look at the distribution of average ratings, we can see that it is very close to a normal distribution. 

- average_rating density plot with normal (without logistic)

Were games to theoretically all have the same mean rating, then the central limit theorem tells us that we would expect a normal distribution. 

Alternatively, it may be that the quality of a game is in general normally distributed (rather than being due to variation in samples)

~~~~~~ [How would we know/determine which of the two reasons are correct, ie. whether games tend to in general have a similar rating (given enough players) vs. whether the game quality acctually varies vs. whether a more rated game tends to be rated ~??[value]7.5 because of the number of people who end up playingit, etc.?]

#### Predicting the average_rating from users_rated
- graph of average_rating vs. users_rated with model fitted
- Model statistics (rmse, rsq)

To predict average_rating from users_rated, we first take the log of users_rated to reduce the skew, and then fit a regression line. As seen, there is a slight positive correlation between log(users_rated) and average_rating meaning that games with few ratings are on average very poorly rated, while any game with a reasonable number of ratings is rated on average fairly similarly. 

~~~~~[should I put numerical equation interpretation in here?]

However, it is clear that this relationship is not very strong, since the RMSE values (give) values are quite high and RSQ quite low. 