---
title: "users_rated"
author: "Arjun NR"
date: "11/21/2020"
output: github_document
editor_options: 
  chunk_output_type: console
---

## Loading packages
```{r load packages}
library(tidyverse)
library(broom)
library(pander)
library(here)
library(tidymodels)
library(fitdistrplus)
```

## Relevant work from proposal
```{r load-data, message = FALSE}
board_games <- read_csv(here("data/board_games.csv"))
```

```{r popular-categories, include = FALSE}
board_games_splitcats <- board_games %>% 
  mutate(categories = str_split(category, ","))

popular_categories <- board_games_splitcats %>%
  pull(categories) %>%
  unlist %>%
  as_tibble %>%
  count(value) %>%
  arrange(desc(n)) %>%
  head(6)
```

```{r no_ratings-v-rating, warning = FALSE}
board_games_splitcats %>%
  ggplot() +
  geom_point(aes(x = users_rated, y = average_rating), alpha = 0.5) +
  scale_x_log10() +
  labs(
  x = "Number of ratings",
  y = "Average ratings",
  title = "Number of ratings against average rating"
  )
```

This plot suggests that games with more ratings tend to have an average rating closer to just under 7.5. However, this may be because there are more games with a rating of 7.5, which increases the likelihood that there will be a game with particularly a large number of ratings. To analyze this further, we will need to eliminate the effect of this confounding variable (number of games with a given rating) to properly analyze the relationship between the two variables. 

## Further work

```{r plus-x-y-densities}
board_games_splitcats %>%
  ggplot() +
  geom_point(aes(x = users_rated, y = average_rating), alpha = 0.05) +
  geom_density(aes(x = users_rated, y = (..scaled.. * 5)), colour = "red") +
  geom_density(aes(x = (..scaled.. * 1e4), y = average_rating), colour = "orange") +
  scale_x_log10() +
  labs(
  x = "Number of ratings",
  y = "Average ratings",
  title = "Number of ratings against average rating", 
  subtitle = "Showing density plots of x and y overlayed"
  ) + 
  scale_colour_viridis_c()
  # xlim(NA, 20000)
```

Below, I have attempted to create a 3D plot of average_rating against users_rated against year_published. It was not particularly informative. 

```{r 3d-plot-n_rate-avge-year, eval=FALSE}
library(plotly)

board_games_splitcats %>% 
  plot_ly(x = ~users_rated, y = ~average_rating, z = ~year_published, type = "scatter3d", mode = "markers", marker = list(color = ~year_published, colorscale = c('#FFE1A1', '#683531'), showscale = TRUE), size = 1, opacity = 0.1)
# plot_ly(x=temp, y=pressure, z=dtime, type="scatter3d", mode="markers", color=temp)
```

### Modelling average_rating from users_rated

```{r av-n-rating-model}
set.seed(314159)
bg_rating_split <- initial_split(board_games_splitcats, prop = 0.8)
train_data <- training(bg_rating_split)
test_data <- testing(bg_rating_split)

bg_rating_fit <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(average_rating ~ log(users_rated), data = train_data)

bg_rating_fit_tidy <- tidy(bg_rating_fit$fit)
bg_rating_fit_aug <- augment(bg_rating_fit$fit)

bg_rating_predictions <- predict(bg_rating_fit, test_data) %>% 
  bind_cols(test_data %>% dplyr::select(average_rating, users_rated, name))

ggplot(dat = bg_rating_predictions, aes(x = users_rated)) + 
  geom_point(aes(y = average_rating)) + 
  geom_line(aes(y = .pred), colour = "red") + 
  scale_x_log10()

# YET TO EVALUATE PERFORMANCE
```

### Fitting a statistical distribution to the distribution of average_rating values

```{r avge_rating-dist-models}
my_data <- board_games_splitcats$average_rating
descdist(my_data, discrete=FALSE, boot=500)

fit <- function(data, distr) {
  fit <- fitdist(data, distr)
  
  print(summary(fit))
  
  return(fit)
}

fit_n  <- fit(my_data, "norm")
fit_ln  <- fit(my_data, "lnorm")
fit_l  <- fit(my_data, "logis")
fit_g  <- fit(my_data, "gamma")

# Following code inspired by http://www.di.fc.ul.pt/~jpn/r/distributions/fitting.html

plot.legend <- c("normal", "lognormal", "logistic", "gamma")

denscomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

cdfcomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

qqcomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

ppcomp(list(fit_n, fit_ln, fit_l, fit_g), legendtext = plot.legend, plotstyle = "ggplot") + theme_minimal()

gofstat(list(fit_n, fit_ln, fit_g, fit_l), fitnames = c("norm", "lnorm", "gamma", "logis"))
```

```{r avge_rating-dist-plot}
sech <- function(x) {
  1 / cosh(x)
}

logis <- function(x, u, s) {
  y <- (1 / (4 * s)) * (sech((x - u) / (2 * s)))^2
  return(y)
}
# inspired by code from https://sebastiansauer.github.io/plotting_s-curve/

logis_av_rating <- function(x) {
  logis(x, fit_l$estimate[1], fit_l$estimate[2])
}

board_games_splitcats %>% 
  ggplot(aes(x = average_rating)) + 
  geom_density() + 
  stat_function(
    fun = dnorm, 
    args = list(
      mean = fit_n$estimate[1], 
      sd = fit_n$estimate[2]
    ), 
    colour = "red", 
    alpha = 0.5
  ) + 
  stat_function(fun = logis_av_rating, colour = "orange", alpha = 0.5) + 
  theme_minimal() + 
  labs(
    title = "Fitting distributtions to distribution of average ratings", 
    subtitle = "red = normal; orange = logistic"
  )
```

### Modelling distribution of average_rating from users_rated

What I am trying to do in next chunk is model the distribution of average_rating depending on users_rated. Thus the steps will likely involve: 

1. Bin users_rated (unless I can do it in a moving average style)
2. Fit normal (or logistic) distr. to average_rating density of each bin (ie. find y-bar and s_y^2)
3. Model normal distr. as function of users_rated (ie. modelling y-bar and s_y^2 as functions of users_rated)

Hopefully will then have a model which gives, for each users_rated, a probability distribution of average_rating

```{r av-n-rating-distr-model, eval = FALSE}
# eval = FALSE as code not yet functioning

bg_rating_rec <- recipe(
  average_rating ~ ., 
  data = train_data
) %>% 
  step_discretize(num_breaks = 10)

# Unfinished
## Is this the right way of going about it? need to look in to this more

#Feedback from Mine:
##in predict() can have confidence interval for each value vs the average - this will give some indication of distribution around the line. Not necessarily good idea to use sd to predict normal distribution as then you are imposing the distribution (guessing it) rather than obtaining it. 
#Can say in project that this is something we could do further (predicting distribution) if give reasons, even if don't necessarily do it. 
#--> something about estimating a prior; bayesian distributions
```